{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PM0uQfVO-bq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return int(sum(labels == predictions) / len(labels) * 100)\n",
    "\n",
    "def normalize(data, feature_range=(0, 1)):\n",
    "\n",
    "    data = np.array(data)\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    return normalized_data, scaler\n",
    "\n",
    "\n",
    "def denormalize(normalized_data, scaler):\n",
    "\n",
    "    normalized_data = np.array(normalized_data)\n",
    "    denormalized_data = scaler.inverse_transform(normalized_data)\n",
    "\n",
    "    return denormalized_data\n",
    "\n",
    "def evaluate_regression(targets, predictions):\n",
    "\n",
    "    targets = np.array(targets)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Compute metrics\n",
    "    mse = mean_squared_error(targets, predictions)  # Mean Squared Error\n",
    "    rmse = np.sqrt(mse)                    # Root Mean Squared Error\n",
    "    r2 = r2_score(targets, predictions)  # R^2 Score\n",
    "\n",
    "    # Return metrics in a dictionary\n",
    "    return {\"RMSE\": rmse, \"R^2\": r2}\n",
    "    \n",
    "def splitData(features, labels, validationPercentage=0.2):\n",
    "    featuresTrain, featuresVal, targetTrain, targetVal = train_test_split(\n",
    "        features,\n",
    "        labels,\n",
    "        test_size=validationPercentage,\n",
    "    )\n",
    "    return featuresTrain, featuresVal, targetTrain, targetVal\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_classification(labels, predictions):\n",
    "    labels = np.array(labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(labels, predictions),\n",
    "        \"Precision\": precision_score(labels, predictions),\n",
    "        \"Recall\": recall_score(labels, predictions),\n",
    "        \"F1 Score\": f1_score(labels, predictions),\n",
    "\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_multiclass_with_one_hot(labelOneHot, predictionOneHot):\n",
    "\n",
    "    # Converte one-hot labels para indices de classes\n",
    "    label = np.argmax(labelOneHot, axis=1)\n",
    "    prediction = np.argmax(predictionOneHot, axis=1)\n",
    "\n",
    "    # Computa as métricas\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(label, prediction),\n",
    "        'precision': precision_score(label, prediction, average='weighted', zero_division=0),\n",
    "    }\n",
    "\n",
    "    print(\"Evaluation Metrics:\")\n",
    "\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "#Activation functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    #Função de ativação usada para normalizar valores em um intervalo de 0 a 1\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    #Calcula a derivada da função sigmoide, útil para o backpropagation\n",
    "    sigmoidResult = sigmoid(x)\n",
    "    return sigmoidResult * (1 - sigmoidResult)\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    #Função de ativação que não modifica a saída do neurônio, útil em tarefas de regressão \n",
    "    return x\n",
    "\n",
    "def identityDerivative(x):\n",
    "    #Derivada da função identidade\n",
    "    return np.ones_like(x)\n",
    "\n",
    "def ReLU(x):\n",
    "    #Função de ativação usada para introduzir não-linearidade. Muito utilizada para neurônios de camadas ocultas.\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def ReluDerivative(x):\n",
    "    #Calcula a derivada de ReLU\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    # usar para classificação multi-classes\n",
    "\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def softmaxDerivative(x):\n",
    "    # Em razão da fórmula de cálculo para cost da categoricalEntropy, não é necessário calcular a derivada da softmax\n",
    "    return 1\n",
    "\n",
    "\n",
    "activationFunctions = {\n",
    "    \"SIGMOID\": (sigmoid, sigmoidDerivative),\n",
    "    \"RELU\": (ReLU, ReluDerivative),\n",
    "    \"SOFTMAX\": (softmax, softmaxDerivative),\n",
    "    \"IDENTITY\": (identity, identityDerivative)\n",
    "}\n",
    "\n",
    "# Cost functions\n",
    "\n",
    "def meanSquaredError(predictions, labels):\n",
    "    # Função de custo erro quadrático. Usar para regressão\n",
    "    return np.mean((predictions - labels) ** 2)\n",
    "\n",
    "\n",
    "def mseDerivative(prediction, label):\n",
    "    # Derivada da mse para uma observação\n",
    "    return 2 * (prediction - label)\n",
    "\n",
    "\n",
    "def binaryCrossEntropy(predictions, labels):\n",
    "    # Calcula o erro de entropia cruzada. Usar para classificação binária\n",
    "\n",
    "    epsilon=1e-15\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon) #limita valores de prediction para evitar logaritmos de 0 ou 1, que resultariam em erros.\n",
    "\n",
    "    return -np.mean(labels * np.log(predictions) + (1 - labels) * np.log(1 - predictions))\n",
    "\n",
    "\n",
    "def binaryEntropyDerivative(prediction, label):\n",
    "    #Calcula a derivada da entropia cruzada binária\n",
    "    epsilon=1e-15\n",
    "    prediction = np.clip(prediction, epsilon, 1 - epsilon) #limita valores de prediction para evitar logaritmos de 0 ou 1, que resultariam em erros.\n",
    "    return (prediction - label) / (prediction * (1 - prediction))\n",
    "\n",
    "\n",
    "def categoricalCrossEntropy(predictions, labels):\n",
    "    # usar para multiclassificação\n",
    "    return -np.sum(labels * np.log(predictions)) / predictions.shape[0] # dividir por predictions.shape[0] tem o intuito de normalizar o valor da perda pelo número de amostras, tornando a perda independente do tamanho do batch\n",
    "\n",
    "\n",
    "def categoricalEntropyDerivative(predictions, labels):\n",
    "    epsilon=1e-12\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    return predictions - labels\n",
    "\n",
    "\n",
    "\n",
    "costFunctions = {\n",
    "    \"MSE\": (meanSquaredError, mseDerivative),\n",
    "    \"BINARY_ENTROPY\": (binaryCrossEntropy, binaryEntropyDerivative),\n",
    "    \"CATEGORICAL_ENTROPY\": (categoricalCrossEntropy, categoricalEntropyDerivative)\n",
    "}\n",
    "\n",
    "#init Layers\n",
    "\n",
    "def initLayers(layers, attrNum):\n",
    "    \"\"\"\n",
    "    Função para inicializar as camadas da rede neural\n",
    "\n",
    "    Recebe:\n",
    "\n",
    "        Layers: array de tuplas. Cada tupla representa uma camada da rede neural e possui o formato (númeroDeNeurônios, idFunçãoDeAtivação)\n",
    "\n",
    "        attrNum: número de atributos da camada de input\n",
    "\n",
    "    Retorna:\n",
    "\n",
    "        initialized_layers: uma lista contendo dicionários de parâmetros para cada camada\n",
    "            - weights: matriz de pesos da camada\n",
    "            - activation: função de ativação da camada\n",
    "            - derivation: função de derivação da camada\n",
    "    \"\"\"\n",
    "\n",
    "    # inicializa prevLayerNeurons com o número de neurônios da camada de input\n",
    "    prevLayerNeurons = attrNum\n",
    "\n",
    "    # inicializa lista de camadas vazia\n",
    "    initialized_layers = []\n",
    "\n",
    "    # Itera por todas as camadas\n",
    "    for i, layer in enumerate(layers):\n",
    "\n",
    "        # inicializa dicionário de parâmetros\n",
    "        layerParams = {}\n",
    "\n",
    "        # Extrai número de neurônios e função de ativação da camada\n",
    "        neuronNum, activation = layer\n",
    "\n",
    "        # Gera a matriz de pesos da camada. Um row por neurônio, contendo um peso para cada neurônio da camada anterior + 1 para o bias\n",
    "        #layerParams[\"weights\"] = np.random.randn(\n",
    "        #    neuronNum, prevLayerNeurons + 1)\n",
    "        layerParams[\"weights\"] = np.random.uniform(low=-0.33, high=0.33,size=(neuronNum,prevLayerNeurons + 1))\n",
    "        \n",
    "        # Extrai funções de ativação e derivação da camada\n",
    "        activationFunction, derivateFunction = activationFunctions[activation]\n",
    "        layerParams[\"activation\"] = activationFunction\n",
    "        layerParams[\"derivation\"] = derivateFunction\n",
    "\n",
    "        # Atualiza prevLayerNeurons com o número de neurônios da camada atual\n",
    "        prevLayerNeurons = neuronNum\n",
    "\n",
    "        # Guarda os parâmetros da camada\n",
    "        initialized_layers.append(layerParams)\n",
    "\n",
    "    return initialized_layers\n",
    "\n",
    "# Backpropagation\n",
    "\n",
    "def backpropagation(layers, costD, learningRate):\n",
    "    \"\"\"\n",
    "\n",
    "    Implementa a operação da retropropagação para toda a rede\n",
    "\n",
    "    Recebe: \n",
    "\n",
    "        layers: lista contendo pesos e funções de ativação/derivadas. Ver initLayers para mais detalhes\n",
    "\n",
    "        costD: derivada da função de custo, já calculada\n",
    "\n",
    "        learningRate: parâmetro de learningRate definido inicialmente\n",
    "\n",
    "    Retorna:\n",
    "\n",
    "        - Lista contendo os pesos ajustados após a retropropagação\n",
    "    \"\"\"\n",
    "\n",
    "    # inicializa nextLayerWeights e nextLayerErrorSignals com None (Começa pela última camada)\n",
    "    nextLayerWeights = None\n",
    "    nextLayerErrorSignals = None\n",
    "\n",
    "    # Inicializa uma lista de pesos ajustados\n",
    "    adjustedWeights = []\n",
    "\n",
    "    # itera pelas camadas (começando pela última e indo até a primeira)\n",
    "    for layerParams in reversed(layers):\n",
    "\n",
    "        # extrai valores intermediários produzidos durante a forwardPass pela camada atual\n",
    "        intermediateValues = layerParams[\"intermediate\"]\n",
    "\n",
    "        # Obtém pesos da camada\n",
    "        weights = layerParams[\"weights\"]\n",
    "\n",
    "        # Obtém função para calcular derivada da função de ativação da camada\n",
    "        activationDerivative = layerParams[\"derivation\"]\n",
    "\n",
    "        # Obtém pesos ajustados e sinais de erro da camada atual\n",
    "        newWeights, errorsSignals = backpropagateLayer(\n",
    "            (weights, nextLayerWeights),\n",
    "            intermediateValues,\n",
    "            learningRate,\n",
    "            costD,\n",
    "            activationDerivative,\n",
    "            nextLayerErrorSignals\n",
    "        )\n",
    "\n",
    "        # Atualiza nextLayerWeights e nextLayerErrorSignals com os valores correspondentes da camada atual\n",
    "        nextLayerWeights = weights\n",
    "        nextLayerErrorSignals = errorsSignals\n",
    "\n",
    "        # Insere os valores dos pesos ajustados na lista de pessoas ajustados, assegurando a ordem correta\n",
    "        adjustedWeights.insert(0, newWeights)\n",
    "\n",
    "    return adjustedWeights\n",
    "\n",
    "\n",
    "def backpropagateLayer(weights, intermediateValues, learningRate, costD, activationDerivative, nextLayerErrorSignals=None):\n",
    "    \"\"\" \n",
    "\n",
    "    Realiza a operação de backpropagação em uma camada\n",
    "\n",
    "    Recebe:\n",
    "\n",
    "        weights: tupla contendo os pesos da layer atual e os pesos da layer seguinte\n",
    "\n",
    "        intermediateValues: tupla contendo os valores intermediários relevantes para a camada (inputs recebidos e combinações produzidas)\n",
    "\n",
    "        learningRate: parâmetro de learningRate\n",
    "\n",
    "        costD: valor calculado para a derivada da função de custo\n",
    "\n",
    "        activationDerivative: função para cálculo da derivada da função de ativação\n",
    "\n",
    "        nextLayerErrorSignals: Error signals da camada seguinte\n",
    "\n",
    "    Retorna:\n",
    "\n",
    "        newWeights: os pesos ajustados após o processo de retropropagação\n",
    "\n",
    "        errorSignals: os sinais de erro produzidos na camada, que serão propagados para a camada anterior na próxima etapa\n",
    "    \"\"\"\n",
    "\n",
    "    # Extrai pesos da camada atual e da camada seguinte\n",
    "    currentWeights, nextLayerWeights = weights\n",
    "\n",
    "    # extrai inputs recebidos pela camada atual e combinações lineares que produziu\n",
    "    layerInput, combinations = intermediateValues\n",
    "\n",
    "    # calcula a derivada da função de ativação da camada\n",
    "    activationD = activationDerivative(combinations)\n",
    "\n",
    "    # Se nextLayerErrorSignals é None, calcula o errorSignals para a camada de output. Do contrário, propaga o error signal para as camadas anteriores\n",
    "    if nextLayerErrorSignals is None:\n",
    "        # camada de output\n",
    "        errorSignals = costD * activationD\n",
    "    else:\n",
    "        # camadas ocultas\n",
    "\n",
    "        # propaga o ErrorSignal da camada seguinte para a camada atual, considerando os pesos das camada seguinte\n",
    "        propagatedErrorSignals = np.dot(\n",
    "            nextLayerErrorSignals, nextLayerWeights[:, 1:])\n",
    "\n",
    "        # Calcula o errorSignal da camada atual, considerando a derivada da função de ativação\n",
    "        errorSignals = propagatedErrorSignals * activationD\n",
    "\n",
    "    # calcula o gradiente da camada atual e multiplica pela learningRate\n",
    "    gradients = np.outer(errorSignals, layerInput) * learningRate\n",
    "\n",
    "    # obtém os pesos da camada atual após a retropropagação\n",
    "    newWeights = currentWeights - gradients\n",
    "\n",
    "    return newWeights, errorSignals\n",
    "\n",
    "# RNA\n",
    "\n",
    "def prepareInput(observation):\n",
    "    \"\"\"\n",
    "    Inclui X0 = 1 no array de observações para multiplicar pelo BIAS.\n",
    "\n",
    "    observation consiste em um array contendo os atributos de uma observação\n",
    "    \"\"\"\n",
    "    return np.concatenate(([1], observation))\n",
    "\n",
    "\n",
    "def forwardPass(input, weights, activationFunction):\n",
    "    \"\"\"\n",
    "    Realizar um forward pass para uma camada\n",
    "\n",
    "    Recebe:\n",
    "\n",
    "        input: Array contendo as entradas da camada (atributos do input ou ativações da camada anterior)\n",
    "\n",
    "        weights: matriz contendo os pesos de cada neurônio da camada\n",
    "\n",
    "        activationFunction: função de ativação definida para uso na camada\n",
    "\n",
    "    Retorna:\n",
    "\n",
    "        activation: os valores calculados para a ativação dos neurônios\n",
    "\n",
    "        combination: os valores calculados na combinação linear dos neurônios\n",
    "    \"\"\"\n",
    "\n",
    "    combination = np.dot(input, weights.T)\n",
    "    activation = activationFunction(combination)\n",
    "\n",
    "    return activation, combination\n",
    "\n",
    "\n",
    "def rna(input, layers):\n",
    "    \"\"\"\n",
    "    Implementa a etapa de forward propagation da rede neural\n",
    "\n",
    "    Recebe:\n",
    "\n",
    "        input: Array contendo os atributos da observação para processamento\n",
    "\n",
    "        layers: lista contendo dicionário com parâmetros de cada camada. Ver initLayers.\n",
    "\n",
    "    Retorna:\n",
    "\n",
    "        activations os valores de ativação calculados pela camada de output\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # inicializa prevActivations com os valores da camada de input\n",
    "    prevActivations = input\n",
    "\n",
    "    # iteração pelas camadas da rede neural\n",
    "    for i in range(len(layers)):\n",
    "\n",
    "        # extrai parâmetros da camada\n",
    "        layerParams = layers[i]\n",
    "\n",
    "        # Extrai os pesos da camada\n",
    "        layerWeights = layerParams[\"weights\"]\n",
    "\n",
    "        # Extrai a função de ativação da camada\n",
    "        activationFunction = layerParams[\"activation\"]\n",
    "\n",
    "        layerInput = prepareInput(prevActivations)\n",
    "\n",
    "        # Realiza a forwardPass da camada\n",
    "        activations, combinations = forwardPass(\n",
    "            layerInput, layerWeights, activationFunction)\n",
    "\n",
    "        # Guarda os valores intermediários produzidos\n",
    "        layerParams[\"intermediate\"] = (layerInput, combinations)\n",
    "\n",
    "        # atualiza prevActivations para os valores de ativação produzidos nessa camada\n",
    "        prevActivations = activations\n",
    "\n",
    "    return activations\n",
    "\n",
    "\n",
    "# Train\n",
    "\n",
    "def train(epochs, learningRate, layers, observations, labels, costF):\n",
    "    \"\"\"\n",
    "\n",
    "    Implementa o algoritmo de treinamento da rede neural.\n",
    "\n",
    "    Recebe:\n",
    "\n",
    "        epochs: número de épocas para treinamento\n",
    "\n",
    "        learningRate: valor do parâmetro de learningRate\n",
    "\n",
    "        layers: lista com os parâmetros de cada camada da rede. Ver initLayers para mais detalhes.\n",
    "\n",
    "        observations: lista de tuplas contendo observações para treinamento. Cada tupla tem dois elementos: ([atributosDaObservação, label])\n",
    "            Exemplo:\n",
    "            observations = [\n",
    "                ([0, 0], 0),\n",
    "                ([0, 1], 1),\n",
    "                ([1, 0], 1),\n",
    "                ([1, 1], 1),\n",
    "                ]\n",
    "\n",
    "        costF: identificador para função de custo. Valores válidos:\n",
    "            MSE\n",
    "            BINARY_ENTROPY\n",
    "            CATEGORICAL_ENTROPY\n",
    "    \"\"\"\n",
    "\n",
    "    error = []\n",
    "    \n",
    "    # extrai a função de custo e sua derivada\n",
    "    costFunction, costDerivative = costFunctions[costF]\n",
    "\n",
    "    # itera pela quantidade de épocas definida\n",
    "    for n in range(epochs):\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        # itera pelas observações\n",
    "        for observation, label in zip(observations, labels):\n",
    "\n",
    "            # formata observations e labels como arrays\n",
    "            observation = np.array(observation)\n",
    "            label = np.array(label)\n",
    "\n",
    "            # Repassa os atributos e parâmetros das camadas para a rede neural e obtém uma predição\n",
    "            prediction = rna(observation, layers)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "            # Calcula a derivada do custo para a observação corrente\n",
    "            costD = costDerivative(prediction, label)\n",
    "\n",
    "            # Obtém os pesos ajustados através da retropropagação\n",
    "            adjustedWeights = backpropagation(\n",
    "                layers, costD, learningRate)\n",
    "\n",
    "            # Atualiza os valores dos pesos usando os valores ajustados\n",
    "\n",
    "            for i, layer in enumerate(layers):\n",
    "                layer[\"weights\"] = adjustedWeights[i]\n",
    "\n",
    "        # invoca a função de custo\n",
    "        predictions = np.array(predictions)\n",
    "        cost = costFunction(predictions, labels)\n",
    "        print(cost)\n",
    "        error.append(cost)\n",
    "\n",
    "    #\n",
    "    plt.plot(error)\n",
    "    if (costF == \"MSE\"):\n",
    "        plt.title(\"regressão - treino\")\n",
    "    elif (costF == \"BINARY_ENTROPY\"):\n",
    "        plt.title(\"classificação binária - treino\")\n",
    "    else:\n",
    "        plt.title(\"classificação multiclasse - treino\")\n",
    "    plt.xlabel(\"epoca\")\n",
    "    plt.ylabel(\"custo\")\n",
    "    plt.show()\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7ukQjOeRNCH"
   },
   "source": [
    "Carrega dados do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__5OhniPRPiz"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/content/penguins.csv'\n",
    "data = np.genfromtxt(DATASET_PATH, delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pA2-sY7RSu3"
   },
   "source": [
    "Separa features e labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRY5XblFRVE1"
   },
   "outputs": [],
   "source": [
    "features = data[:, :-3]\n",
    "labels = data[:, -3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISUkaV7VRW88"
   },
   "source": [
    "Separa dados para treinamento e para validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDxP3hMbRagF"
   },
   "outputs": [],
   "source": [
    "featuresTrain, featuresVal, labelsTrain, labelsVal = splitData(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0Q-tzTjRd9T"
   },
   "source": [
    "Normaliza features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yME8mchBRgK1"
   },
   "outputs": [],
   "source": [
    "normalized_features, feature_scaler = normalize(featuresTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqMAzGoGRlpf"
   },
   "source": [
    "Define a estrutura da rede, funções de ativação, funções de custo, épocas e learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3T35_6GRtye"
   },
   "outputs": [],
   "source": [
    "# Obtém a quantidade de neurons da camada de input\n",
    "INPUT_NEURONS = len(normalized_features[0])\n",
    "\n",
    "# Define a estrutura da rede e funções de ativação\n",
    "layers = [\n",
    "    (INPUT_NEURONS, 'RELU'),\n",
    "    #(INPUT_NEURONS, 'RELU'),\n",
    "    (3, 'SOFTMAX')\n",
    "]\n",
    "\n",
    "# Definição função de custo, épocas e learning rate\n",
    "COSTF = \"CATEGORICAL_ENTROPY\"\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Inicializa os pesos e as funções das camadas:\n",
    "\n",
    "layers = initLayers(layers, INPUT_NEURONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5K5PdJVR1fL"
   },
   "source": [
    "Dispara o treinamento da rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZ0h2vWkR5kx"
   },
   "outputs": [],
   "source": [
    "trainedParams = train(EPOCHS, LEARNING_RATE, layers, normalized_features, labelsTrain, COSTF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FiR8cU2R6PD"
   },
   "source": [
    "Normaliza as features para validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75pETASaR-dD"
   },
   "outputs": [],
   "source": [
    "test_normalized_features = feature_scaler.transform(featuresVal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6Zkle-RSBsf"
   },
   "source": [
    "Testa a rede treinada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mk9q5ANXSE5a"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i, observation in enumerate(test_normalized_features):\n",
    "\n",
    "  input = observation\n",
    "  prediction = rna(input, trainedParams)\n",
    "\n",
    "  one_hot_prediction = np.zeros_like(prediction)\n",
    "  one_hot_prediction[np.argmax(prediction)] = 1\n",
    "\n",
    "  predictions.append(one_hot_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9umz2_T5SHW_"
   },
   "source": [
    "Obtém métricas de avaliação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svlXly6ISLHa",
    "outputId": "9810c767-5126-4f4f-c2b8-fa79df3fb210"
   },
   "outputs": [],
   "source": [
    "metrics = evaluate_multiclass_with_one_hot(labelsVal, predictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
